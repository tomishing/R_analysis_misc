[
  {
    "objectID": "penguin_sample.html",
    "href": "penguin_sample.html",
    "title": "penguins",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "penguin_sample.html#quarto",
    "href": "penguin_sample.html#quarto",
    "title": "penguins",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "penguin_sample.html#running-code",
    "href": "penguin_sample.html#running-code",
    "title": "penguins",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "penguin.html",
    "href": "penguin.html",
    "title": "Classification of Penguin Species using Machine Learning Models",
    "section": "",
    "text": "Exclusively distributed in the Southern Hemisphere\nEighteen species are recognized\n\n\n\n\nPenguins distribution\n\n\n(source: Romos et al. BMC Genomics (2018) 1953, DOI 10.1186/s12864-017-4424-9; https://en.wikipedia.org/wiki/List_of_penguins)\n\n\n\n\n\n\n\n\n\nAdelie\n\n\n\n\n\n\n\nChinstrap\n\n\n\n\n\n\n\nGentoo\n\n\n\n\n\n\n\n\n\nAdelie distribution\n\n\n\n\n\n\n\nChinstrap distribution\n\n\n\n\n\n\n\nGentoo distribution\n\n\n\n\n\n(source: Andrew Shiva/Wikipedia; POLAR LITERACY, https://tinyurl.com/3h4xcr6n)\n\n\n\nGorman et al. (2014) provide part of their research data on three penguin species, aiming to investigate whether environmental variability is associated with differences in male and female pre-breeding foraging niches. Using this dataset, we examine whether habitat, morphological traits, and other features can help distinguish the species."
  },
  {
    "objectID": "penguin.html#introduction",
    "href": "penguin.html#introduction",
    "title": "Classification of Penguin Species using Machine Learning Models",
    "section": "",
    "text": "Exclusively distributed in the Southern Hemisphere\nEighteen species are recognized\n\n\n\n\nPenguins distribution\n\n\n(source: Romos et al. BMC Genomics (2018) 1953, DOI 10.1186/s12864-017-4424-9; https://en.wikipedia.org/wiki/List_of_penguins)\n\n\n\n\n\n\n\n\n\nAdelie\n\n\n\n\n\n\n\nChinstrap\n\n\n\n\n\n\n\nGentoo\n\n\n\n\n\n\n\n\n\nAdelie distribution\n\n\n\n\n\n\n\nChinstrap distribution\n\n\n\n\n\n\n\nGentoo distribution\n\n\n\n\n\n(source: Andrew Shiva/Wikipedia; POLAR LITERACY, https://tinyurl.com/3h4xcr6n)\n\n\n\nGorman et al. (2014) provide part of their research data on three penguin species, aiming to investigate whether environmental variability is associated with differences in male and female pre-breeding foraging niches. Using this dataset, we examine whether habitat, morphological traits, and other features can help distinguish the species."
  },
  {
    "objectID": "penguin.html#methods",
    "href": "penguin.html#methods",
    "title": "Classification of Penguin Species using Machine Learning Models",
    "section": "Methods",
    "text": "Methods\n\nData\nThe data were originally collected by Gorman and her collegues (see Gorman KB et al. (2014) PLOS ONE, 9(3): e90081. https://doi.org/10.1371/journal.pone.0090081 ) and were downloaded from Kaggle for the years 2021 to 2025 (https://tinyurl.com/mmy6ds5n). This dataset is already well curated, it contains no missing values, duplicates or null entries.\nData were collected in the three islands: Biscoe, Dream and Torgensen in Antarctica.\n (source: Gorman et al. 2014)\n\n\nImporting and exploring data\n\nlibrary(pacman)\np_load(tidyverse, skimr, gridExtra, GGally, corrplot, car)\n\nWe used the pacman library, which allows installing and loading libraries simultaneously, as an alternative to using install.packages() and library() separately.\n\ndf &lt;- read.csv(\"palmerpenguins_extended.csv\")\n\n\nExploring data\n\ndf %&gt;% glimpse\n\nRows: 3,430\nColumns: 11\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Biscoe\", \"Biscoe\", \"Biscoe\", \"Biscoe\", \"Biscoe\", \"B…\n$ bill_length_mm    &lt;dbl&gt; 53.4, 49.3, 55.7, 38.0, 60.7, 35.7, 61.0, 66.1, 61.4…\n$ bill_depth_mm     &lt;dbl&gt; 17.8, 18.1, 16.6, 15.6, 17.9, 16.8, 20.8, 20.8, 19.9…\n$ flipper_length_mm &lt;dbl&gt; 219, 245, 226, 221, 177, 194, 211, 246, 270, 230, 27…\n$ body_mass_g       &lt;dbl&gt; 5687, 6811, 5388, 6262, 4811, 5266, 5961, 6653, 6722…\n$ sex               &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"female\", \"f…\n$ diet              &lt;chr&gt; \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fis…\n$ life_stage        &lt;chr&gt; \"adult\", \"adult\", \"adult\", \"adult\", \"juvenile\", \"juv…\n$ health_metrics    &lt;chr&gt; \"overweight\", \"overweight\", \"overweight\", \"overweigh…\n$ year              &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021…\n\n\nThe dataset contains both numerical and categorical columns. It has 10 feartuers and 1 target column (it is the species column in this report), and 3,430 records.\n\nskim_s &lt;- skim(df)\nprint(skim_s)\n\n── Data Summary ────────────────────────\n                           Values\nName                       df    \nNumber of rows             3430  \nNumber of columns          11    \n_______________________          \nColumn type frequency:           \n  character                6     \n  numeric                  5     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable  n_missing complete_rate min max empty n_unique whitespace\n1 species                0             1   6   9     0        3          0\n2 island                 0             1   5   9     0        3          0\n3 sex                    0             1   4   6     0        2          0\n4 diet                   0             1   4   8     0        4          0\n5 life_stage             0             1   5   8     0        3          0\n6 health_metrics         0             1   7  11     0        3          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable     n_missing complete_rate   mean      sd     p0    p25    p50\n1 bill_length_mm            0             1   38.5   13.2    13.6   28.9   34.5\n2 bill_depth_mm             0             1   18.4    2.77    9.1   16.6   18.4\n3 flipper_length_mm         0             1  207.    28.9   140    185    203  \n4 body_mass_g               0             1 4835.  1311.   2477   3844.  4634. \n5 year                      0             1 2023.     1.31 2021   2022   2024  \n     p75    p100 hist \n1   46.6    88.2 ▃▇▃▂▁\n2   20.3    27.9 ▁▃▇▃▁\n3  226     308   ▂▇▅▂▁\n4 5622   10549   ▆▇▃▁▁\n5 2024    2025   ▃▆▆▇▇\n\n\n\n\nTarget\nThe target variable is penguin species, consisting of three classes, making this a multiclass classification problem. The dataset is slightly imbalanced, though not enough to require adjustment. Adelie penguins represented in the largest number of records, while Chinstrap penguins have the fewest.\n\ntable(df$species)\n\n\n   Adelie Chinstrap    Gentoo \n     1560       623      1247 \n\n\n\n\nNumeric data distribution\n\ndf$year &lt;- as.factor(df$year)\nnumeric_df &lt;- df[sapply(df, is.numeric)]\ndf_num_tar &lt;- data.frame(species = df$species, year = df$year, \n                         numeric_df)\n\n\ndf_l &lt;- df_num_tar %&gt;% pivot_longer(cols = 3:6, \n                        names_to = \"variables\",  values_to = \"values\")\np &lt;- df_l %&gt;% ggplot(aes(x = species, y = values)) + geom_boxplot() +\n                  facet_wrap(~variables, scales = \"free_y\")\np\n\n\n\n\n\n\n\n\nThese figures show the outliers in all numerical features, bill depth, bill length, body mass and flipper length. Theoretically such outliers should be removed before applying linear models or distance-based models. However, I decided to keep them for now. If the performance of those models is poor or requires improvements, we will revisit this issue.\nThat said, data collected through fieldworks in harsh environment are rare, and the outliers often carry valuable ecological information. Therefore, they should not be removed unless these are clearly measurement errors or artificial anomalies.\n\n\nCategorical data\n\ndf_char &lt;- df[sapply(df, function(x) is.character(x) || is.factor(x) )]\ndf_l &lt;- df_char %&gt;% pivot_longer(cols = 1:7, names_to = \"variables\", values_to = \"values\")\n\n\n  df_l %&gt;%  ggplot(aes(x = factor(values))) + geom_bar() +\n  facet_wrap(~ variables, scales = \"free_y\") +\n  labs(x=\"\", y = \"\") + \n  coord_flip() \n\n\n\n\n\n\n\n\nThe number of individuals differs across the categorical features except for sex. The largest number of measured penguins comes from the Biscoe island.\n\nSpecies vs Island\n\np &lt;- df %&gt;% ggplot(aes(island, fill = species)) + geom_bar(position = \"stack\")\np + labs(x = \"Island\", y = \"The number of Penguins\") + theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWe can observe distinct habitat differences among species. Chinstrap and Gentoo penguins are found mainly on Biscoe and Dream islands, respectively, whereas Adelie penguins inhabit all three islands.\n\n\n\nPair plot of numeric data\n\ncor_mat &lt;- cor(numeric_df[-5], use = \"complete.obs\")\ncorrplot(cor_mat, \n         method = \"color\",\n         type = \"upper\",\n         addCoef.col = \"black\",\n         tl.col = \"black\",\n         tl.srt = 40)\n\n\n\n\n\n\n\n\nThere are clear correlations among morphological variables. In particular, the correlation between body mass and flipper length appears to be high.\n\nmodel &lt;- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm, data = df)\nvif(model)\n\n   bill_length_mm     bill_depth_mm flipper_length_mm \n         1.811190          1.312078          2.142848 \n\n\nHowever, the VIF values are all below 5, indicating that collinearity is negligible.\n\n\nThe relationship between flipper length and weight\n\np1 &lt;- df %&gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + geom_point() +\n  labs(x = \"Weight (g)\", y = \"Flipper length (mm)\") + theme_minimal(base_size = 14)\n\np2 &lt;- df %&gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = sex)) + geom_point() +\n  labs(x = \"Weight (g)\", y = \"Flipper length (mm)\") + theme_minimal(base_size = 14)\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nWhile there is a clear relationship between flipper length and weight, differences between sexes and among species are not easily distinguishable.\n\n\nPair plot for numerical data\n\ndf01 &lt;- data.frame(species = df$species, numeric_df)\ndf01 %&gt;% ggpairs(ggplot2::aes(color = species, alpha = 0.4))\n\n\n\n\n\n\n\n\nThe ggpairs() covers most of preliminary examinations we have done up to this point, making it useful for quickly grasping an overall picture of the data.\n\n\n\nAnalytical methods\n\nModels\nWe used the classification models listed below. Except for selecting the value of k in kNN, we did not extensively tune hyperparameters. Our assumption was that if a model is well-suited to the data, it can still perform reasonably well with default settings. Therefore, the hyperparameters used here are essentially the default values, which are typically chosen to be broadly effective. Thus, the performance comparisons presented below are based on these default hyperparameter settings.\n\nLogistic regression models with Losso, Ridge, and Elastic net regularizations\nDecision Tree\nNaive Bayes\nK-Nearest Neighbors (KNN)\nSupport Vector Machine (SVM) (Linear)\nSupport Vector Machine (KSVM) (Radial Basis kernel “Gaussian”)\nRandom Forest\nBagging\nXGBoost\n\n\nFor each model, the workflow is as follows:\n\nModeling\nPrediction\nEvaluation\n\n\nConfusion matrix\nMetrics: accuracy, precision, recall, F1-score\nROC and AUC\n\n\n\n\nLoading libraries for machine learning\n\np_load(caret, rpart, rpart.plot, e1071, class, kernlab, MLmetrics,\n       randomForest, ipred, xgboost, glmnet, pROC)\n\n\n\nConvert data type of species from character to factor\nSince R does not automatically treat character variables as categorical, we need to convert character columns to factors before analysis.\n\ndf &lt;- df %&gt;%\n  mutate(across(where(is.character), as.factor))\n\n\n\nLabel Encoding and Standardization\nBecause data standardization is essential for SVM and kNN, and also recommended for logistic regression, we created an additional dataset in which all features were encoded and scaled.\n\nStandardization\n\n# df0 &lt;- df %&gt;% mutate_if(is.numeric, ~scale(.))\ndf0 &lt;- df %&gt;% mutate_if(is.numeric, \\(x)scale(x))\n\n\n\nLabel Encoding\nLabel encoding is also essential for SVM and kNN.\n\ndummy &lt;- model.matrix(species ~ . -1, data = df0)\ndf_svm_knn &lt;- data.frame(species = df0$species, dummy)\n\n\n\n\nPartitioning\nSplitting the dataset into training and test sets is an essential step for a machine learning pipeline. Although several methods are available across different libraries, we use createDataPartition() from the caret library.\n\nThe dataset for the tree-based models and Naive Bayes.\n\n\nset.seed(123)\nsample &lt;- createDataPartition(df$species, p = 0.7, list = FALSE)\ntrain &lt;- df[sample, ]\ntest &lt;- df[-sample, ]\n\n\nThe datasets for SVM, kNN and Logistic Regression models.\n\n\nset.seed(223)\nsample &lt;- createDataPartition(df_svm_knn$species, p = 0.7, list = FALSE)\ntrain_svmknn &lt;- df_svm_knn[sample, ]\ntest_svmknn &lt;- df_svm_knn[-sample, ]\n\n\n\n\nModeling\n\nDecision Tree, Naive Bayes, Random Forest, Bagging, XGBoost\n\nDataset: df, train, test\n\n\nDecision Tree\n\n#model_dt &lt;- rpart(\n#  species ~ ., data = train, method = \"class\",\n#  control = rpart.control(cp = 0.01, minsplit = 10, maxdepth = 5)\n#)\nmodel_dt &lt;- rpart(\n  species ~ ., data = train, method = \"class\"\n)\n\n\n\nNaive Bayes\nNaive Bayes assumes that all predictors are conditionally independent, an assumption that is violated when predictors are highly correlated. Typically, |r| &lt; 0.7 is considered acceptable for this model. Because the correlation between body mass and flipper length exceeds 0.7, this may reduce reliability of the analysis.\n\nmodel_bayes &lt;- naiveBayes(species ~ ., data = train, laplace = 1)\n\n\n\nRandom Forest\n\nset.seed(123)\nmodel_rf &lt;- randomForest(species ~., data = train, \n                        ntree = 500, mtry = 6, importance = TRUE, \n                        na.action = na.roughfix,replace = FALSE)\n\nHere,\n\nntree: the number of trees in the forest. Default value is 500.\n‘mtry’: the number of variables randomly sampled at each split, which controls tree diversity.\nimportance computes variable importance measures after training.\nna.action = na.roughfix handles missing values. It replaces NA with median for numeric variables and mode for factors.\nreplace controls whether bootstrap sampling is used.\n\n\n\nBagging\n\nset.seed(123)\nmodel_bg &lt;- bagging(species ~., data = train)\n\n\n\nXGBoost\n\nxgboost() requires numerical data in matrix form.\n\ndoes not assume intercept\nassume that the labels starts from 0\n\nmodel.matrix(() converts a model formula and data frame into a numerical design matrix.\n\nConverts categorical variables (factors) into dummy variables\nAutomatically remove the response variable (dependent or target variable)\n\n\n\nset.seed(123)\nX &lt;- model.matrix(species ~ . - 1, data = train) # remove intercept\ny &lt;- as.numeric(train$species) - 1 # the labeles starts from 0\n\nmodel_xg &lt;- xgboost(\n    data = X, \n    label = y, \n    nrounds = 10,\n    objective = \"multi:softprob\",\n    num_class = length(unique(y))\n    )\n\n[1] train-mlogloss:0.868836 \n[2] train-mlogloss:0.728109 \n[3] train-mlogloss:0.629022 \n[4] train-mlogloss:0.557720 \n[5] train-mlogloss:0.500952 \n[6] train-mlogloss:0.457259 \n[7] train-mlogloss:0.418241 \n[8] train-mlogloss:0.383407 \n[9] train-mlogloss:0.359887 \n[10]    train-mlogloss:0.337747 \n\n\n\n\n\nLogistic, kNN and SVM\n\nDataset: df_svm_knn, train_svmknn, test_svmknn\n\n\nLogistic regression model with Losso, Ridge, and Elastic net regularizations\n\n‘cv.glmnet’ assume a numerical matrix as input.\n\nHowever, it does not need to start the labels from 0\nAlso, it can include intercept by default.\n\n\n\nX_lg &lt;- model.matrix(species ~ ., train_svmknn) \ny_lg &lt;- train_svmknn$species\n\ncv_fit &lt;- cv.glmnet(\n    X_lg, y_lg,\n    family = \"multinomial\",\n    type.measure = \"class\",\n    nfolds = 5,\n    alpha = 0.5      # 1 = LASSO, 0 = Ridge 0.5 = Elastic net\n)\n\n# Best model\nbest_lambda &lt;- cv_fit$lambda.min\n\n# Fit final model\nmodel_lg &lt;- glmnet(X_lg, y_lg, family = \"multinomial\", \n                   alpha = 0.5, lambda = best_lambda)\n\n\n\nkNN\nThe best k is estimated.\n\nset.seed(123)\n\nk &lt;- seq(1, 21, by = 2)\ntune_grid &lt;- expand.grid(k = k)\n\nknn_tune &lt;- train(species ~., data = train_svmknn, method = \"knn\", \n                  trControl = trainControl(method = \"cv\", number = 5),\n                  tuneGrid = tune_grid\n                  )\n\nprint(paste0(\"k = \", knn_tune$bestTune))\n\n[1] \"k = 21\"\n\n\n\ntrain_knn &lt;- train_svmknn[-1] # Remove target\ntest_knn &lt;- test_svmknn[-1] # Remove target\n\nmodel_knn &lt;- knn(train = train_knn, test = test_knn, \n                 cl = train_svmknn$species, k = knn_tune$bestTune)\n\n\n\nSVM\nWe tested two types of SVM kernels: the linear kernel and the RBF kernel, with the latter serving as a non-linear alternative.\n\ntrain_svmknn$species &lt;- as.factor(train_svmknn$species)\nmodel_svm &lt;- svm(species ~ ., data = train_svmknn, kernel=\"linear\")\n\n\n\nKSVM\n\nmodel_ksvm &lt;- ksvm(species~., data = train_svmknn, kernel = \"rbfdot\", \n                   prob.model = TRUE)\n\n\n\n\n\nFeature Importance Calculation\nThe variable-importance results from the three models, Decision Tree, Random Forest, and XGBoost, consistently indicate that the island variable is most important predictor.\n\nDecision tree\n\n\nimp &lt;- model_dt$variable.importance\nbarplot(sort(imp, decreasing = TRUE), las = 2, \n        main = \"Decision Tree\")\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\nvarImpPlot(model_rf, main = \"Random Forest\")\n\n\n\n\n\n\n\n\n\nXGBoost\n\n\nimp &lt;- xgb.importance(feature_names = colnames(X), model = model_xg)\nxgb.plot.importance(imp[1:10], main = \"XGBBoost\")\n\n\n\n\n\n\n\n\n\n\nThe decision tree figure\n\nrpart.plot(model_dt)\n\n\n\n\n\n\n\n\nThe decision tree plot shows that the model appropriately classified Gentoo, but struggled to distinguish Chinstrap from Adelie. For example, Chinstrap penguins are not found on Torgensen island.\n\n\nPrediction\n\nDecision Tree\n\npred_dt &lt;- predict(model_dt, test, type = \"class\")\n\n\n\nNaive Bayes\n\npred_nb &lt;- predict(model_bayes, test)\n\n\n\nRandom Forest\n\npred_rf &lt;- predict(model_rf, test)\n\n\n\nBagging\n\npred_bg &lt;- predict(model_bg, test)\n\n\n\nXGBoost\n\nX_test  &lt;- model.matrix(species ~ . - 1, data = test)\ny_test  &lt;- as.numeric(test$species) - 1\n\n# returns a flatter vector of probabilities\npred_prob_xg &lt;- predict(model_xg, X_test) \n# comvert it into a matrix form\npred_prob_xg01 &lt;- matrix(pred_prob_xg, ncol = length(unique(y)), \n                         byrow = TRUE)\n# convert into labels in a vector.\npred_class_xg &lt;- max.col(pred_prob_xg01) - 1 \n\n\n\nLogistic models\n\nx_test &lt;- model.matrix(species ~ ., data = test_svmknn)\ny_test &lt;- as.numeric(test$species) \n\npred_class_lg &lt;- predict(model_lg, x_test, type = \"class\") \npred_prob_lg &lt;- predict(model_lg, x_test, type = \"response\")\n\n\n\nkNN\n\npred_knn &lt;- model_knn\n\n\n\nSVM\n\npred_svm &lt;- predict(model_svm, test_svmknn)\n\n\n\nKSVM\n\npred_ksvm &lt;- predict(model_ksvm, test_svmknn)"
  },
  {
    "objectID": "penguin.html#results-evaluation",
    "href": "penguin.html#results-evaluation",
    "title": "Classification of Penguin Species using Machine Learning Models",
    "section": "Results: Evaluation",
    "text": "Results: Evaluation\n\nConfusion matrix\nCreate a list of confusion matrices for each model.\n\nclass_names &lt;- levels(train$species)\ntrue_label_xg  &lt;- class_names[y_test]\npred_label_xg  &lt;- class_names[pred_class_xg + 1]\n\npred_models &lt;- list(pred_dt, pred_nb, pred_rf, pred_bg, \n                    pred_label_xg,  pred_class_lg, pred_knn, \n                    pred_svm, pred_ksvm)\nmodel_names &lt;- c(\"Decision Tree\", \"Naive Bayes\", \"Random Forest\", \n                 \"Bagging\", \"XGBoost\", \"Logistic\", \n                 \"kNN\", \"SVM\", \"KSVM\")\n\ntable_list &lt;- map(pred_models, \\(pred){\n  actual_class &lt;- test$species\n  if (identical(pred, pred_class_lg) |\n      identical(pred, pred_knn) |\n      identical(pred, pred_svm) |\n      identical(pred, pred_ksvm)) {\n    actual_class &lt;- test_svmknn$species\n  }\n  \n  if(identical(pred, pred_label_xg)){\n    actual_class &lt;- true_label_xg\n  }\n  tb &lt;- table(pred, actual_class)\n  df_tb &lt;- tb %&gt;% as.data.frame()\n  colnames(df_tb) &lt;- c(\"predicted\", \"actual\", \"freq\")\n  df_tb\n})\nnames(table_list) &lt;- model_names\n\nCreate a list of confusion matrix plots for each model.\n\nplot_list &lt;- map(model_names, \\(ml){\n  df10 &lt;- table_list[[ml]]\n  df10 %&gt;% \n  ggplot(aes(x = actual, y = predicted, fill = freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = freq), size = 5) +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  scale_y_discrete(limits = rev) +        # reverse y-axis\n  scale_x_discrete(position = \"top\") +    # move x-axis to top\n  labs(x = ml, y = \"Predicted\") + \n  theme_minimal(base_size = 14) + \n  theme(legend.position=\"none\") \n})\n\nUse do.call() to call a function and pass its arguments as a list for creating confusion matrix plots.\n\ndo.call(grid.arrange, c(plot_list, ncol = 3))\n\n\n\n\n\n\n\n\nThe figures show that the Logistic Regression model and SVM models perform better than the other models.\n\n\nAccuracy, CE and F1\n\nAccuracy\n\nAccuracy = (Number of corect predictions) / (Total number of predictions)\nEasy to understand, but misleading with imbalanced classes.\n\nF1 Score\n\nF1 = 2 x (Precision x Recall) / (Presicion + Recall)\nPrecision = (Correct positive predictions) / (Total predicted positives)\nRecall: (Correct positive predictions) / (Total actural positives)\nUseful when classes are imbalanced\n\nClassification Error (CE)\n\nCE = 1 - Accuracy\n\n\n\n# Function to select correct actual labels per model\nget_actual &lt;- function(pred_vec) {\n  if (identical(pred_vec, pred_class_lg) |\n      identical(pred_vec, pred_knn) |\n      identical(pred_vec, pred_svm) |\n      identical(pred_vec, pred_ksvm)) {\n    return(test_svmknn$species)\n  } else if (identical(pred_vec, pred_label_xg)) {\n    return(true_label_xg)\n  } else {\n    return(test$species)\n  }\n}\n\n# Compute results programmatically\nresults_df &lt;- imap_dfr(pred_models, \\(pred, i) {\n\n  actual_class &lt;- get_actual(pred)\n\n  # Ensure factors have same levels\n  pred_vec &lt;- factor(pred, levels = levels(actual_class))\n\n  ac  &lt;- Accuracy(pred, actual_class) %&gt;% round(3)\n  f1  &lt;- F1_Score(pred, actual_class) %&gt;% round(3)\n  ce  &lt;- round(1 - ac, 3)\n\n  tibble(\n    Model = model_names[i],\n    Accuracy = ac,\n    F1 = f1,\n    CE = ce\n  )\n})\n\n\nresults_df %&gt;% arrange(-Accuracy)\n\n# A tibble: 9 × 4\n  Model         Accuracy    F1    CE\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SVM              0.88  0.863 0.12 \n2 Logistic         0.877 0.861 0.123\n3 KSVM             0.807 0.765 0.193\n4 XGBoost          0.772 0.73  0.228\n5 Random Forest    0.768 0.737 0.232\n6 Bagging          0.758 0.725 0.242\n7 kNN              0.746 0.691 0.254\n8 Decision Tree    0.725 0.655 0.275\n9 Naive Bayes      0.658 0.635 0.342\n\n\nBased on Accuracy, F1, and CE, SVM is the best-performing model, followed by Logistic Regression.\n\n\nROC and AUC\nThe Receiver Operating Characteristic (ROC) curve illustrates the performance of a binary classifier across all possible threshold values. The curve depicts the trade-off between sensitivity (True Positive Rate, TPR) and 1 − specificity (False Positive Rate, FPR).\nFor multiclass classification, we applied a one-vs-rest (OvR) approach, creating a binary dataset for each penguin species. For example, in the Adelie penguins classifier, all Adelie penguins are labeled 1, while the other penguins are labeled 0. This allows us to compute an ROC curve and corresponding AUC value for each species.\nThe Area Under the Curve (AUC) quantifies the area under the ROC curve, summarizing the model’s overall ability to discriminate between classes.\n\nROC using pROC library\n\n# p_load(pROC)\n\n\n# prob_dt &lt;- predict(model_dt, test, type = \"prob\")\n\n\n#model_svm &lt;- svm(species ~ ., data = train_svmknn, kernel=\"linear\", \n#                 probability = TRUE)\n#pred_svm &lt;- predict(model_svm, test_svmknn, probability = TRUE)\n#prob_svm &lt;- attr(pred_svm, \"probabilities\")\n#\n#mc_roc &lt;- multiclass.roc(test$species, prob_svm)\n#\n\n\n\nONE-vs-REST\n\nProbability\n\n# decision tree\nprob_dt &lt;- predict(model_dt, test, type = \"prob\")\n\n# bayes\nprob_bayes &lt;- predict(model_bayes, test, type = \"raw\")\n\n# Random forest\nprob_rf &lt;- predict(model_rf, test, type = \"prob\")\n\n# Bagging\nprob_bg &lt;- predict(model_bg, test, type = \"prob\")\n\n# XGBoost\npred_prob_xg &lt;- predict(model_xg, X_test)\nnum_class &lt;- length(levels(train$species))\nprob_xg &lt;- matrix(pred_prob_xg, ncol = num_class, byrow = TRUE)\ncolnames(prob_xg) &lt;- levels(train$species)\n\n# Logistic model\nprob_list &lt;- predict(model_lg, x_test, type = \"response\")\nprob_lg &lt;- prob_list[, , 1]\ncolnames(prob_lg) &lt;- levels(train_svmknn$species)\n\n# knn\nmodel_knn3 &lt;- knn3(species ~ ., data = train)\nprob_knn &lt;- predict(model_knn3, test, type = \"prob\")\n\ntrain_svmknn$species &lt;- as.factor(train_svmknn$species)\n\n# svm\nmodel_svm &lt;- svm(species ~ ., data = train_svmknn, kernel=\"linear\",\n                 probability = TRUE)\npred_svm &lt;- predict(model_svm, test_svmknn, probability = TRUE)\nprob_svm &lt;- attr(pred_svm, \"probabilities\")\n\n# ksvm\nmodel_ksvm &lt;- ksvm(species ~ ., data = train_svmknn, \n                   kernel = \"rbfdot\",  prob.model = TRUE)\nprob_ksvm &lt;- predict(model_ksvm, test_svmknn, type = \"probabilities\")\n\n\n\nROC\nCalculate the ROC curves for all models separately for each penguin species.\n\nmodels &lt;- list(\n  dt   = prob_dt,\n  bayes = prob_bayes,\n  knn  = prob_knn,\n  svm  = prob_svm,\n  ksvm = prob_ksvm,\n  random = prob_rf,\n  bagging = prob_bg,\n  xgb = prob_xg,\n  logistic = prob_lg\n)\n\ntruth &lt;- test_svmknn$species %&gt;% as.factor() # must be factor\nclasses &lt;- levels(test_svmknn$species %&gt;% as.factor())\nroc_list &lt;- list() \n\nfor (m_name in names(models)){\n  prob_df &lt;- models[[m_name]] %&gt;% as.data.frame()\n  for (cls in classes){\n    binary_truth &lt;- ifelse(truth == cls, 1, 0)\n    roc_obj &lt;- roc(binary_truth, prob_df[[cls]], quiet = TRUE)\n    roc_list[[paste(m_name, cls, sep = \"_\")]] &lt;- roc_obj\n  }\n}\n\nConvert the roc_list into data frames and visualize them as plots.\n\nroc_df &lt;- map_df(names(roc_list), \\(nm) {\n  roc_obj &lt;- roc_list[[nm]]\n  df &lt;- data.frame(\n    specificity = roc_obj$specificities,\n    sensitivity = roc_obj$sensitivities,\n    name = nm\n  )\n  df\n})\n\nroc_df &lt;- roc_df %&gt;%\n  separate(name, into = c(\"Model\", \"Class\"), sep = \"_\")\n\nggplot(roc_df, aes(x = 1 - specificity, y = sensitivity, \n                   color = Model)) +\n  geom_line(size = 1) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", \n              color = \"gray\") +\n  facet_wrap(~ Class) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"ROC Curves by Model and Class\",\n    x = \"False Positive Rate\",\n    y = \"True Positive Rate\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nApparently, Logistic Regression and SVM performs better across all penguin species.\n\n\nAUC\nExtract the AUC values for each model from roc_list and compile them into a data frame.\n\nauc_values &lt;- map_dbl(roc_list, \\(x)x$auc) %&gt;% round(3)\nauc_df &lt;- data.frame(models = auc_values %&gt;% names, \n                     AUC = auc_values %&gt;% as.numeric())\n\nVisualize the AUC values for all models.\n\n# Calculate average AUC values by models\n\nauc_df &lt;- auc_df %&gt;% \n  mutate(label = auc_df$models) %&gt;%\n  separate(models, into = c(\"model\", \"class\"), sep = \"_\") %&gt;%\n  group_by(model) %&gt;% \n  mutate(mean_AUC = mean(AUC)) %&gt;% \n  ungroup()\n\n# Order the MODEL factor by mean_AUC (this controls legend order)\nauc_df$model &lt;- factor(auc_df$model, levels = auc_df %&gt;% \n                         distinct(model, mean_AUC) %&gt;% \n                         arrange(-mean_AUC) %&gt;% \n                         pull(model))\n\n# Order the bar labels (x-axis)\nauc_df &lt;- auc_df %&gt;% \n  arrange(mean_AUC, AUC, model) %&gt;% \n  mutate(label = factor(label, levels = label))\n\n# Plot\nggplot(auc_df, aes(x = label, y = AUC, fill = model)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() +\n  theme(axis.title.y = element_blank())\n\n\n\n\n\n\n\n\nThis figure also shows that Logistic Regression and SVM models outperform the other models."
  },
  {
    "objectID": "penguin.html#conclusion",
    "href": "penguin.html#conclusion",
    "title": "Classification of Penguin Species using Machine Learning Models",
    "section": "Conclusion",
    "text": "Conclusion\nPredicting species from habitat, morphological traits, and other features was possible with high accuracy. The Logistic Regression and SVM models achieved accuracies greater than 0.87, with average AUC values for each species exceeding 0.96. In contrast, other models, including tree-based, distance-based and probalistic approaches, were less effective for these predictions.\nEven so, compared to the other species, the best-performing models still struggled to distinguish Adelie penguins from the other two species. Considering the variable importance indicated by the tree-based models, this difficulty may be related to their habitats: while Chinstrap and Gentoo penguins inhabit specific islands, Adelie are found on all three islands."
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "cluster",
    "section": "",
    "text": "Cluster analysis is a method for uncovering structure in unlabelled data by grouping similar observations together. It is a fundamental technique in data science and is widely applied across research, industry, and business analytics.\nThere are several commonly used clustering methods. In this essay, I introduce the following:\n\nK-means clustering\n\n\nPartitions data into k clusters\nMinimizes within-cluster variance\nFast and widely used\nRequire k in advance\n\n\nPAM (Partitioning Around Medoids)\n\n\nsimilar to k-means but more robust, especially with noise and outliers\nPAM groups data into k clusters by selecting actual data points, called medoids, as the centers of clusters.\nRequire k in advance\n\n\nHierarchical clustering\n\n\nBuilds a tree (dendrogram) based on distance Two types:\nAgglomerative (bottom-up)\nDivisive (top-down)\nDoes not require specifying the number of clusters at first.\n\n\nDensity-based clustering (DBSCAN)\n\n\nGroups points based on density\nDetects arbitrary-shaped clusters\nGood for finding outliers\nDoes not require precising number of clusters.\n\n\nSelf-Organizing Map (SOM)\n\n\nA neural-network–based clustering and visualization technique\nServes as both a clustering algorithm and a nonlinear dimensionality reduction tool\nMaps high-dimensional data onto a 2D grid while preserving similarity\nCommon applications include customer segmentation, image recognition, gene expression analysis, ecological data, market research, and remote sensing/GIS.\n\nFor methods such as k-means, where the number of clusters k must be chosen beforehand, several techniques have been developed to determine an appropriate value:\n\nElbow method\nSilhouette method\nPAMK\nNbClust\n\nIn this essay, after introducing k-means and these approaches for selecting k, I present examples of PAM, hierarchical clustering, and DBSCAN, followed by a demonstration of SOM."
  },
  {
    "objectID": "cluster.html#introduction-cluster-analysis",
    "href": "cluster.html#introduction-cluster-analysis",
    "title": "cluster",
    "section": "",
    "text": "Cluster analysis is a method for uncovering structure in unlabelled data by grouping similar observations together. It is a fundamental technique in data science and is widely applied across research, industry, and business analytics.\nThere are several commonly used clustering methods. In this essay, I introduce the following:\n\nK-means clustering\n\n\nPartitions data into k clusters\nMinimizes within-cluster variance\nFast and widely used\nRequire k in advance\n\n\nPAM (Partitioning Around Medoids)\n\n\nsimilar to k-means but more robust, especially with noise and outliers\nPAM groups data into k clusters by selecting actual data points, called medoids, as the centers of clusters.\nRequire k in advance\n\n\nHierarchical clustering\n\n\nBuilds a tree (dendrogram) based on distance Two types:\nAgglomerative (bottom-up)\nDivisive (top-down)\nDoes not require specifying the number of clusters at first.\n\n\nDensity-based clustering (DBSCAN)\n\n\nGroups points based on density\nDetects arbitrary-shaped clusters\nGood for finding outliers\nDoes not require precising number of clusters.\n\n\nSelf-Organizing Map (SOM)\n\n\nA neural-network–based clustering and visualization technique\nServes as both a clustering algorithm and a nonlinear dimensionality reduction tool\nMaps high-dimensional data onto a 2D grid while preserving similarity\nCommon applications include customer segmentation, image recognition, gene expression analysis, ecological data, market research, and remote sensing/GIS.\n\nFor methods such as k-means, where the number of clusters k must be chosen beforehand, several techniques have been developed to determine an appropriate value:\n\nElbow method\nSilhouette method\nPAMK\nNbClust\n\nIn this essay, after introducing k-means and these approaches for selecting k, I present examples of PAM, hierarchical clustering, and DBSCAN, followed by a demonstration of SOM."
  },
  {
    "objectID": "cluster.html#data",
    "href": "cluster.html#data",
    "title": "cluster",
    "section": "Data",
    "text": "Data\nData is obtained through Wine Quality Data Set (Red & White Wine). The data is originally prepared and kindly allowed to shared by P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009. The detail such as the number of records, features and data type is as follow:\n\nlibrary(pacman)\np_load(tidyverse, skimr)\ndf &lt;- read.csv(\"wine-quality-white-and-red.csv\")\n\ndf %&gt;% skim\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n6497\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n0\n1\n3\n5\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nfixed.acidity\n0\n1\n7.22\n1.30\n3.80\n6.40\n7.00\n7.70\n15.90\n▂▇▁▁▁\n\n\nvolatile.acidity\n0\n1\n0.34\n0.16\n0.08\n0.23\n0.29\n0.40\n1.58\n▇▂▁▁▁\n\n\ncitric.acid\n0\n1\n0.32\n0.15\n0.00\n0.25\n0.31\n0.39\n1.66\n▇▅▁▁▁\n\n\nresidual.sugar\n0\n1\n5.44\n4.76\n0.60\n1.80\n3.00\n8.10\n65.80\n▇▁▁▁▁\n\n\nchlorides\n0\n1\n0.06\n0.04\n0.01\n0.04\n0.05\n0.06\n0.61\n▇▁▁▁▁\n\n\nfree.sulfur.dioxide\n0\n1\n30.53\n17.75\n1.00\n17.00\n29.00\n41.00\n289.00\n▇▁▁▁▁\n\n\ntotal.sulfur.dioxide\n0\n1\n115.74\n56.52\n6.00\n77.00\n118.00\n156.00\n440.00\n▅▇▂▁▁\n\n\ndensity\n0\n1\n0.99\n0.00\n0.99\n0.99\n0.99\n1.00\n1.04\n▇▂▁▁▁\n\n\npH\n0\n1\n3.22\n0.16\n2.72\n3.11\n3.21\n3.32\n4.01\n▁▇▆▁▁\n\n\nsulphates\n0\n1\n0.53\n0.15\n0.22\n0.43\n0.51\n0.60\n2.00\n▇▃▁▁▁\n\n\nalcohol\n0\n1\n10.49\n1.19\n8.00\n9.50\n10.30\n11.30\n14.90\n▃▇▅▂▁\n\n\nquality\n0\n1\n5.82\n0.87\n3.00\n5.00\n6.00\n6.00\n9.00\n▁▆▇▃▁\n\n\n\n\n\n\ndf %&gt;% glimpse\n\nRows: 6,497\nColumns: 13\n$ type                 &lt;chr&gt; \"white\", \"white\", \"white\", \"white\", \"white\", \"whi…\n$ fixed.acidity        &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0, 6.3, 8.1,…\n$ volatile.acidity     &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32, 0.27, 0…\n$ citric.acid          &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16, 0.36, 0…\n$ residual.sugar       &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00, 20.70,…\n$ chlorides            &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.058, 0.050, 0.045, …\n$ free.sulfur.dioxide  &lt;dbl&gt; 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 11, 17, 1…\n$ total.sulfur.dioxide &lt;dbl&gt; 170, 132, 97, 186, 186, 97, 136, 170, 132, 129, 6…\n$ density              &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0.9951, 0…\n$ pH                   &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.18, 3.00, 3…\n$ sulphates            &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47, 0.45, 0…\n$ alcohol              &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.8, 9.5, 11…\n$ quality              &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6…\n\n\n\np_load(dbscan, cluster, fpc, NbClust, kohonen)"
  },
  {
    "objectID": "cluster.html#cluster-analysis",
    "href": "cluster.html#cluster-analysis",
    "title": "cluster",
    "section": "Cluster analysis",
    "text": "Cluster analysis\nCluster analyses that rely on distance measures usually require the data to be standardized beforehand. Methods such as k-means, PAM, hierarchical clustering, NbClust, and SOM all depend on distance calculations, making standardization an essential preprocessing step.\nIn addition, distance-based analyses require categorical variables to be converted into numeric form, such as through label encoding\n\ndf_scale &lt;- df %&gt;% mutate(across(where(is.numeric), scale)) # standardization\nmt_st &lt;- model.matrix( ~. -1, data = df_scale) # label encoding\n\n\nK-means\n\nChoose the best k\n\nElbow method\n\nwss &lt;- sapply(1:20, function(k){\n  kmeans(mt_st, centers = k, nstart = 25)$tot.withinss\n})\n\nplot(1:20, wss, type = \"b\",\n     xlab = \"Number of clusters (k)\",\n     ylab = \"Total within-cluster sum of squares\",\n     main = \"Elbow Method\")\n\n\n\n\n\n\n\n\n\n\nSilhouette method\nThe best k is the one with the highest silhouette width.\n\nsil_width &lt;- sapply(2:10, \n                    \\(x){pam(mt_st, k = x)$silinfo$avg.width\n                      })\nplot(2:10, sil_width, type = \"b\",\n     xlab = \"Number of clusters (k)\",\n     ylab = \"Average silhouette width\",\n     main = \"Silhouette Method\")\n\n\n\n\n\n\n\n\n\n\nPAMK and PAM\n\npamk_res &lt;- pamk(mt_st, krange = 2:10)\npamk_res$nc\n\n[1] 2\n\n\n\nlayout(matrix(c(2,1), 1, 2))\nplot(pamk_res$pamobject)\n\n\n\n\n\n\n\nlayout(matrix(1))\n\n\npam_res &lt;- pam(mt_st, k = 2)\n# Cluster plot\nclusplot(pam_res, main = \"PAM Clustering\", color = TRUE, shade = TRUE, labels = 2, lines = 0)\n\n\n\n\n\n\n\n\nThis figure is identical to the plot on the right in the previous set of plots.\n\n\nNbClust\n\nset.seed(123)\nmt2 &lt;- mt_st[, -1] # remove multicollinearity\nnb_d &lt;- NbClust(\n    data = mt2,\n    min.nc = 2,\n    max.nc = 10,\n    method = \"kmeans\"\n)\n\n\n\n\n\n\n\n\n*** : The Hubert index is a graphical method of determining the number of clusters.\n                In the plot of Hubert index, we seek a significant knee that corresponds to a \n                significant increase of the value of the measure i.e the significant peak in Hubert\n                index second differences plot. \n \n\n\n\n\n\n\n\n\n\n*** : The D index is a graphical method of determining the number of clusters. \n                In the plot of D index, we seek a significant knee (the significant peak in Dindex\n                second differences plot) that corresponds to a significant increase of the value of\n                the measure. \n \n******************************************************************* \n* Among all indices:                                                \n* 4 proposed 2 as the best number of clusters \n* 15 proposed 3 as the best number of clusters \n* 1 proposed 7 as the best number of clusters \n* 1 proposed 8 as the best number of clusters \n* 1 proposed 9 as the best number of clusters \n* 1 proposed 10 as the best number of clusters \n\n                   ***** Conclusion *****                            \n \n* According to the majority rule, the best number of clusters is  3 \n \n \n******************************************************************* \n\n\n\nnb_d$Best.nc \n\n                    KL       CH Hartigan     CCC   Scott      Marriot   TrCovW\nNumber_clusters 3.0000    3.000    3.000 10.0000     3.0 3.000000e+00        3\nValue_Index     7.4088 1728.936 1300.309 59.9161 15579.4 1.698081e+45 25913398\n                  TraceW Friedman   Rubin Cindex     DB Silhouette   Duda\nNumber_clusters     3.00   3.0000  3.0000 8.0000 3.0000     3.0000 2.0000\nValue_Index     10602.48  46.3789 -0.2266 0.1188 1.6643     0.2205 1.9475\n                 PseudoT2   Beale Ratkowsky    Ball PtBiserial Frey McClain\nNumber_clusters     2.000  2.0000     3.000     3.0     3.0000    1  2.0000\nValue_Index     -1735.905 -4.3215     0.337 15685.1     0.4553   NA  0.7061\n                  Dunn Hubert SDindex Dindex   SDbw\nNumber_clusters 7.0000      0  3.0000      0 9.0000\nValue_Index     0.0147      0  1.3126      0 0.6041\n\n\nThe optimal number of clusters appears to be 3. In addition, the Silhouette index calculated by NbClust is 3.\n\n\n\nClustering\nConduct k-means clustering with the number of clusters set to 2.\n\nkm_res &lt;- kmeans(mt_st, centers = 2, nstart = 25)\n\nVisualization using PCA.\n\npca &lt;- prcomp(mt_st, scale. = TRUE)\npca_df &lt;- data.frame(PC1 = pca$x[ , 1], PC2 = pca$x[ , 2], Cluster = factor(km_res$cluster))\nplot(pca_df[c(\"PC1\", \"PC2\")], col = pca_df$Cluster)\n\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering\n\nd &lt;- dist(mt_st, method = \"euclidean\")\nhc &lt;- hclust(d, method = \"complete\")\nhc\n\n\nCall:\nhclust(d = d, method = \"complete\")\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 6497 \n\n\n\nplot(hc)\n\n\n\n\n\n\n\n\nGrouping by distance\n\nplot(hc)\nrect.hclust(hc, k = 2, border = 1:10)\n\n\n\n\n\n\n\n\n\np_load(factoextra)\n\nfviz_cluster(list(data = mt_st, cluster = cutree(hc, k = 2)), labelsize = 7)\n\n\n\n\n\n\n\n\nSingle method\n\nhc2 &lt;- hclust(d, method = \"ward.D2\")\nhc2\n\n\nCall:\nhclust(d = d, method = \"ward.D2\")\n\nCluster method   : ward.D2 \nDistance         : euclidean \nNumber of objects: 6497 \n\n\n\nplot(hc2)\nrect.hclust(hc2, k = 2, border = 1:4)\n\n\n\n\n\n\n\n\n\nfviz_cluster(list(data = mt_st, cluster = cutree(hc2, k = 2)), labelsize = 7)\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\nkNNdistplot(mt_st, k = ncol(mt_st) + 1)\nabline(h = 1.8, col = \"green\")\n\n\n\n\n\n\n\n\n\nx &lt;- 1.8\ndb1 &lt;- dbscan::dbscan(mt_st, eps = x, minPts = ncol(mt_st) + 1)\n\n\nhullplot(mt_st, db1)\n\n\n\n\n\n\n\n\n\nplot(mt_st, col = db1$cluster)\n\n\n\n\n\n\n\n\n\nRemoving noise\n\nnoise &lt;- which(db1$cluster == 0)\ndf2 &lt;- mt_st[-noise,]\nnrow(df2)\n\n[1] 5779\n\n\n\nkNNdistplot(df2, k = ncol(df2) + 1)\nabline(h = 1.75, col = \"red\")\n\n\n\n\n\n\n\n\n\nx &lt;- 1.75\ndb2 &lt;- dbscan::dbscan(df2, eps = x, minPts = ncol(df2) + 1)\nhullplot(df2, db2)\n\n\n\n\n\n\n\n\n\n\n\nSOM\n\nset.seed(123)\nsample_df &lt;- mt_st[sample(1:nrow(mt_st), 1000), ] \nwine_s &lt;- sample_df \ni_grid &lt;- somgrid(xdim = 6, ydim = 6, topo = \"hexagonal\")\n\nset.seed(100)\nsom_model &lt;- som(X = wine_s, grid = i_grid, rlen = 500, alpha = c(0.6, 0.01))\n\n\nCounts\n\nplot(som_model, type = \"counts\")\n\n\n\n\n\n\n\n\n\n\nChanges plot\n\nplot(som_model, type=\"changes\")\n\n\n\n\n\n\n\n\n\n\ndist neighbours plot\n\nplot(som_model, type=\"dist.neighbours\")\n\n\n\n\n\n\n\n\n\n\nCodes\n\nplot(som_model, type=\"codes\")\n\n\n\n\n\n\n\n\n\n\nMapping\n\nplot(som_model, type=\"mapping\")\n\n\n\n\n\n\n\n\n\n\nProperty\n\npar(mfrow=c(1,2))\n\ncoolBlueHotRed &lt;- function(n, alpha = 1){\n  rainbow(n, end=4/6, alpha=alpha)[n:1]}\nfor (i in 1:2){\nplot(som_model, type = \"property\",\n  property = getCodes(som_model)[, i], main = colnames(sample_df)[i],\n  palette.name = coolBlueHotRed)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\n\nfor (i in 3:4){\nplot(som_model, type = \"property\",\n  property = getCodes(som_model)[, i], main = colnames(sample_df)[i],\n  palette.name = coolBlueHotRed)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nfor (i in 5:6){\nplot(som_model, type = \"property\",\n  property = getCodes(som_model)[, i], main = colnames(sample_df)[i],\n  palette.name = coolBlueHotRed)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nfor (i in 7:8){\nplot(som_model, type = \"property\",\n  property = getCodes(som_model)[, i], main = colnames(sample_df)[i],\n  palette.name = coolBlueHotRed)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nfor (i in 9:10){\nplot(som_model, type = \"property\",\n  property = getCodes(som_model)[, i], main = colnames(sample_df)[i]\n  , palette.name = coolBlueHotRed)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nfor (i in 11:12){\nplot(som_model, type = \"property\",\n  property = getCodes(som_model)[, i], main = colnames(sample_df)[i]\n  , palette.name = coolBlueHotRed)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nfor (i in 13:14){\nplot(som_model, type = \"property\",\n  property = getCodes(som_model)[, i], main = colnames(sample_df)[i]\n  , palette.name = coolBlueHotRed)\n}\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\n\nThe grid size appears appropriate because each node contains more than five samples, as shown in the counts plot.\nThe neighbour distance plot suggests the presence of clusters, although the boundaries are not very distinct. However, the codes plot provides a clearer picture. For example, the nodes in the upper-right region share similar profiles, indicating that they belong to the same cluster. The density patterns in the mapping plot also help identify clusters. The densely occupied area in the upper-right region is consistent with what is seen in the neighbour distance and codes plots.\nThe property heatmaps illustrate relationships between features. For instance, comparing the heatmaps of chlorides and free sulfur dioxide suggests a negative correlation between them. In contrast, chlorides and volatile acidity appear to be positively correlated."
  }
]